# Warehouse Navigation using PPO
## Reinforcement Learning Project

---

## ğŸ“ Abstract
This project implements a Reinforcement Learning (RL) agent that navigates a 2D warehouse grid environment containing both static and dynamic obstacles. A custom `WarehouseEnv` simulates shelves, robots, and a goal location. The agent is trained using **Proximal Policy Optimization (PPO)** from Stable Baselines3.

The project covers environment design, reward shaping, PPO architecture, training checkpoints, and evaluation results.

---

## ğŸ“Œ Features
- Custom Gym-compatible warehouse environment  
- Dynamic + static obstacles  
- PPO-based training  
- Training checkpoints  
- Visual results and navigation behavior  
- Fully simulated (no external dataset)

---

## ğŸš€ Problem Statement
The RL agent must:
- Reach the goal  
- Avoid static shelves  
- Avoid dynamic robots  
- Learn a safe and efficient navigation policy  

The observation space includes:
- Robot position  
- Goal position  
- Warehouse obstacle map  

---

## ğŸ§  RL Training Process
Training uses **Stable Baselines3 PPO**, interacting with the custom environment.

### ğŸ¯ PPO Objective
```
L_CLIP(Î¸) = E[min(r_t(Î¸) Ã‚_t, clip(r_t(Î¸), 1âˆ’Îµ, 1+Îµ) Ã‚_t)]
```
Where:
- `r_t(Î¸)` = probability ratio  
- `Ã‚_t` = GAE advantage  
- `Îµ` = clipping parameter  

This stabilizes training by preventing overly large updates.

---

## ğŸ® Reward Structure
| Event | Reward |
|-------|--------|
| Collision | Large negative |
| Step taken | Small negative |
| Goal reached | Large positive |

### GAE Advantage Calculation
```
Î´_t = r_t + Î³V(s_(t+1)) â€“ V(s_t)
Ã‚_t = Î£ (Î³Î»)^l Î´_(t+l)
```

---

## ğŸ‹ï¸ Training Steps
1. Agent interacts with environment â†’ collects (state, action, reward, next state)  
2. Compute GAE advantages  
3. Update actor & critic networks  
4. Save model checkpoints every *50,000* timesteps  
5. Continue until convergence  

---

## ğŸ“‚ Training Checkpoints
- **Initial model** â€“ random behavior  
- **50k, 100k, 150k** â€“ improvements in reward and collision reduction  
- **Final model** â€“ robust navigation with efficient pathing  

---

## âœ… Outcome
The PPO agent:
- Reaches the goal consistently  
- Avoids shelves and moving robots  
- Learns short, efficient paths  
- Shows stability and convergence  

---

## ğŸ“· Results
The following outputs were generated during evaluation:
- Initial grid layout  
- Movement across grid  
- Dynamic obstacle avoidance  
- Final goal reach  

(Place images or GIFs here if available)

---

## ğŸ“¦ Project Structure
```
ğŸ“ warehouse-rl-ppo
 â”œâ”€â”€ warehouse_env.py
 â”œâ”€â”€ train.py
 â”œâ”€â”€ evaluate.py
 â”œâ”€â”€ checkpoints/
 â”œâ”€â”€ README.md
```

---

## ğŸ Conclusion
This project demonstrates the application of **Proximal Policy Optimization** for warehouse navigation.  
It forms a strong base for:
- Multi-agent navigation  
- Real robot deployment  
- More advanced warehouse automation systems  

---

## ğŸ‘¥ Authors
- Samridh Ramesha  
- Ansh Goyal  
- Chandan Singh  
- Bhanodhay Rao  

---

